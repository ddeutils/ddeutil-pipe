# Case:
#   Extract data from a Source Database to Azure Data Lake.
#
#   * The source table should dynamic picking with schema and table names
#   * Incremental loading with ``run_date`` parameter
# ---
ingest_postgres_to_minio:
  type: pipeline.Pipeline
  desc: Extract and download data from Postgres to Azure Data Lake
  params:
    run-date: utils.receive.datetime
    source: utils.receive.string
    target: utils.receive.string
  jobs:
    extract-load:
      stages:
        - name: Extract & Load
          id: extract-load
          task: tasks.el.PostgresToDelta
          args:
            source:
              conn: ${{ source.conn }}
              query: |
                select *
                from  {{ params.source.schema }}.{{ param.source.table }}
                where update_date = '{{ param.run_date.fmt('%Y%m%d') }}'
            conversion:
              customer_id: id
              customer_name: name
              customer_register_date: register_date
              customer_revenue: revenue
            sink:
              conn: ${{ target.conn }}
              endpoint: /persisted/${{ target.schema }}/${{ target.path }}


ingest_local_to_local:
  type: pipeline.Pipeline
  params:
    run-date: utils.receive.datetime
    source: utils.receive.string
    target: utils.receive.string
  jobs:
    extract-load:
      stages:
        - name: "Extract & Load Local System"
          id: extract-load
          task: tasks/el-csv-to-parquet@polars
          args:
            source: "ds_csv_local_file"
            sink: "ds_parquet_local_file"
