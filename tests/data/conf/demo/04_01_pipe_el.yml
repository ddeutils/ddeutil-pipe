# Case:
#   Extract data from a Source Database to Azure Data Lake.
#
#   * The source table should dynamic picking with schema and table names
#   * Incremental loading with ``run_date`` parameter
ingest_postgres_to_adls:
  version: '1'
  type: pipeline.Pipeline
  desc: Extract and download data from Postgres to Azure Data Lake
  params:
    run_date: ${{ ... }}
    source: ${{ ... }}
    target: ${{ ... }}
  jobs:
    extract-load:
      stages:
        - name: Extract & Load
          uses: tasks.copy.Polars:postgres-delta
          with:
            source:
              conn: ${{ source.conn }}
              query: |
                select *
                from  {{ params.source.schema }}.{{ param.source.table }}
                where update_date = '{{ param.run_date.fmt('%Y%m%d') }}'
            conversion:
              customer_id -> id
              customer_name -> name
              customer_register_date -> register_date
              customer_revenue -> revenue
            sink:
              conn: ${{ target.conn }}
              endpoint: /persisted/${{ target.schema }}/${{ target.path }}
              format: parquet
